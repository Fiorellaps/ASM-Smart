{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c6093f-653c-497f-b7a7-d510a2c5cfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./provas-env/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in ./provas-env/lib/python3.9/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: joblib in ./provas-env/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: click in ./provas-env/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./provas-env/lib/python3.9/site-packages (from nltk) (2023.10.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/data/jupyter-lab/provas-env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install haralyzer\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install flashtext\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ccaf8e-7428-43e1-8d30-7df4d40b6685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 * [1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e62929-47e9-4b68-81b6-5e6df399f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haralyzer import HarParser, HarPage\n",
    "import glob\n",
    "from http import HTTPStatus\n",
    "from bs4 import BeautifulSoup\n",
    "from flashtext.keyword import KeywordProcessor\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "import pymssql\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "from elasticsearch.helpers import bulk\n",
    "from elasticsearch.helpers import BulkIndexError\n",
    "\n",
    "from cryptography.fernet import Fernet # generar Fernet key\n",
    "import configparser # Parsear el fichero de configuración\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11535fae-a0ab-4930-9b72-2cdc5bb3d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_elastic():\n",
    "    elastic_url = 'https://10.210.4.119:9200'\n",
    "    elastic_user = 'elastic'\n",
    "    elastic_password = 'eilv2Dh04gbCBYAouc1o'\n",
    "\n",
    "    es = Elasticsearch(\n",
    "        elastic_url,\n",
    "        #ca_certs= elastic_crt,\n",
    "        basic_auth=(elastic_user, elastic_password),\n",
    "        timeout=300,\n",
    "        verify_certs=False\n",
    "    )\n",
    "    return es\n",
    "\n",
    "def run_elastic_query(elastic_connection, query, index_name, page_size=10000, scroll=\"6m\"):\n",
    "    # Disable Elasticsearch scroll logs\n",
    "    #logging.getLogger(\"elasticsearch\").setLevel(logging.WARNING)\n",
    "\n",
    "    \n",
    "    page_size = page_size\n",
    "    scroll_id = None\n",
    "\n",
    "    response = scan(\n",
    "            elastic_connection,\n",
    "            query=query,\n",
    "            index=index_name,\n",
    "            scroll=scroll,\n",
    "            size=page_size\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for hit in response:\n",
    "        results.append(hit[\"_source\"])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def insert_df_to_elastic(elastic_connection, df, index_name):\n",
    "    # Disable Elasticsearch scroll logs\n",
    "    #logging.getLogger(\"elasticsearch\").setLevel(logging.WARNING)\n",
    "    \n",
    "    data_json = df.to_dict(orient='records')\n",
    "    actions = [\n",
    "        {\n",
    "            \"_index\": index_name,\n",
    "            \"_source\":  {key: value for key, value in record.items() if key != \"id\"},\n",
    "            \"_id\": record[\"id\"]\n",
    "        }\n",
    "        for record in data_json\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        bulk(elastic_connection, actions, refresh=True)\n",
    "        \n",
    "    except BulkIndexError as e:\n",
    "        for error in e.errors:\n",
    "            print(\"Indexing failed for document:\", error['index'])\n",
    "            print(\"Reason:\", error['error']['reason'])\n",
    "\n",
    "\n",
    "def get_unique_idTest_from_elastic(index_name):\n",
    "    aggs = {\n",
    "    \"unique_idTest\": {\n",
    "      \"terms\": {\n",
    "        \"field\": \"idTest.keyword\",\n",
    "          \"size\": 10000\n",
    "          }\n",
    "        }\n",
    "     }\n",
    "    result = es.search(index=index_name, size = 0, aggs=aggs)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_raw_asm_test_data(es, index_name, idTest, data_ini, data_fi):\n",
    "\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "              \"must\": [\n",
    "                {\n",
    "                  \"match\": {\n",
    "                    \"idTest\": str(idTest)\n",
    "                  }\n",
    "                },\n",
    "                {\n",
    "                  \"range\": {\n",
    "                    \"fechaProcesado\": {\n",
    "                      \"gte\": data_ini,\n",
    "                      \"lte\": data_fi \n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    data = run_elastic_query(es, query, index_name)\n",
    "\n",
    "    if(len(data) > 0):\n",
    "        data_df = pd.DataFrame(data)\n",
    "\n",
    "        if not 'idError' in data_df.columns:\n",
    "            print(\"Sin idError\")\n",
    "            data_df['idError'] = 0\n",
    "            print(data_df['idError'])\n",
    "        if not 'pathError' in data_df.columns:\n",
    "            print(\"Sin pathError\")\n",
    "            data_df['pathError'] = \"\"\n",
    "        if not 'errorMsg' in data_df.columns:\n",
    "            print(\"Sin errorMsg\")\n",
    "            data_df['errorMsg'] = \"\"\n",
    "        if not 'urlFailed' in data_df.columns:\n",
    "            print(\"Sin urlFailed\")\n",
    "            data_df['urlFailed'] = \"\"\n",
    "        if not 'errorSel' in data_df.columns:\n",
    "            print(\"Sin errorSel\")\n",
    "            data_df['errorSel'] = \"\"\n",
    "        if not 'elementError' in data_df.columns:\n",
    "            print(\"Sin elementError\")\n",
    "            data_df['elementError'] = \"\"\n",
    "            \n",
    "        data_df = data_df.rename(columns={'type': 'tipo'})\n",
    "        data_df['fecha'] = data_df['fecha'].apply(lambda x: datetime.strptime(x, '%Y%m%d%H%M%S'))\n",
    "        data_df['tiempoPaso'] = data_df['tiempoPaso'].astype(float)\n",
    "        data_df['idError'] =  data_df['idError'].fillna(0)\n",
    "        data_df['pathError'] = data_df['pathError'].fillna(\"\")\n",
    "        data_df['errorMsg'] = data_df['errorMsg'].fillna(\"\")\n",
    "        data_df['urlFailed'] = data_df['urlFailed'].fillna(\"\")\n",
    "        data_df['harFile'] = data_df['urlFailed'].fillna(\"\")\n",
    "        data_df['errorSel'] = data_df['errorSel'].fillna(\"\")\n",
    "        data_df['elementError'] = data_df['elementError'].fillna(\"\")\n",
    "        data_df['idError'] = data_df['idError'].astype(int)\n",
    "\n",
    "    else:\n",
    "        data_df = pd.DataFrame()\n",
    "    return data_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236177e0-d2ff-4680-bbd3-ae7a99b88ca1",
   "metadata": {},
   "source": [
    "# Procesamiento de URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08f17c0f-7b45-4c4c-b855-e88b67056331",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"raw-asm\"\n",
    "es = connect_to_elastic()\n",
    "unique_idTest_list = get_unique_idTest_from_elastic(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5c280-5b68-41ff-9ce8-f3c4f304452d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438834ac-b54d-43d4-b5e5-7d1b56b4e096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime_ini 2023-12-04 10:38:15\n",
      "datetime_fi 2023-12-05 10:38:15\n"
     ]
    }
   ],
   "source": [
    "# Definición de variables\n",
    "datetime_ini = datetime.now() - timedelta(days=2)\n",
    "datetime_ini = str(datetime_ini) \n",
    "datetime_ini = datetime.strptime(datetime_ini, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "datetime_ini_str = datetime_ini.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"datetime_ini\", datetime_ini_str)\n",
    "\n",
    "datetime_fi = datetime.now() - timedelta(days=1)\n",
    "datetime_fi = str(datetime_fi)\n",
    "datetime_fi = datetime.strptime(datetime_fi,\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "datetime_fi_str = datetime_fi.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"datetime_fi\", datetime_fi_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aaaa249-2995-4b1e-ac19-cd4d615e9eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin pathError\n",
      "Sin errorMsg\n",
      "Sin urlFailed\n",
      "Sin errorSel\n",
      "Sin elementError\n"
     ]
    }
   ],
   "source": [
    "for hit in unique_idTest_list[\"aggregations\"][\"unique_idTest\"]['buckets'][1:2]:\n",
    "    idTest = hit['key']\n",
    "    data_current_df = get_raw_asm_test_data(es, index_name, idTest, datetime_ini_str, datetime_fi_str)\n",
    "    \n",
    "    for index, row in data_current_df.iterrows():\n",
    "        if row['urlFailed'] != \"\":\n",
    "            #print(row['urlFailed'], row['errorSel'],row['errorMsg'],row['idPaso']  )\n",
    "            row['errorSel'].split('?')\n",
    "            url_error_dict = {\n",
    "                \"base_url\": row['urlFailed'].split('?')[0],\n",
    "                \"url\": row['urlFailed'],\n",
    "                \"error_selenium\": row['errorSel'].replace(\"]\", \"\"),\n",
    "                \"error_message\": row['errorMsg'],\n",
    "                \"paso\": row['idPaso']\n",
    "            }\n",
    "            print(url_error_dict)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd0dbc-a017-4066-a8b1-91d8c78dbc91",
   "metadata": {},
   "source": [
    "1) Generar los datos\n",
    "2) Generar el modleo\n",
    "3) Clasificar \n",
    "4) Comparar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "816b1980-ee60-4c70-8b0c-3b2b137983b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_percentage(value_0, value_x):\n",
    "    try:\n",
    "        total_percentage = float(value_x)/float(value_0)\n",
    "        total_percentage = total_percentage * 100\n",
    "    except:\n",
    "        return 0\n",
    "    else:\n",
    "        return total_percentage\n",
    "\n",
    "\n",
    "def get_text_category(text_from_html):\n",
    "    \n",
    "    text=str(text_from_html).lower()\n",
    "    number_class_0 = len(all_keywords_processor.extract_keywords(text))\n",
    "    number_class_1 = len(home_page_keywords_processor.extract_keywords(text))\n",
    "    number_class_2 = len(login_keywords_processor.extract_keywords(text))\n",
    "    number_class_3 = len(register_keywords_processor.extract_keywords(text))\n",
    "    number_class_4 = len(services_keywords_processor.extract_keywords(text))\n",
    "    number_class_5 = len(computer_keyword_processor.extract_keywords(text))\n",
    "    number_class_6 = len(profile_keyword_processor.extract_keywords(text))\n",
    "    number_class_7 = len(form_keyword_processor.extract_keywords(text))\n",
    "    number_class_8 = len(banners_keyword_processor.extract_keywords(text))\n",
    "    number_class_9 = len(contact_keyword_processor.extract_keywords(text))\n",
    "    number_class_10 = len(security_keyword_processor.extract_keywords(text))\n",
    "    number_class_11 = len(social_networks_keyword_processor.extract_keywords(text))\n",
    "    number_class_12 = len(mobile_app_keyword_processor.extract_keywords(text))\n",
    "    \n",
    "    total_matches = number_class_0\n",
    "    \n",
    "    percentage_1 = float(get_match_percentage(number_class_0, number_class_1))\n",
    "    percentage_2 = float(get_match_percentage(number_class_0, number_class_2))\n",
    "    percentage_3 = float(get_match_percentage(number_class_0, number_class_3))\n",
    "    percentage_4 = float(get_match_percentage(number_class_0, number_class_4))\n",
    "    percentage_5 = float(get_match_percentage(number_class_0, number_class_5))\n",
    "    percentage_6 = float(get_match_percentage(number_class_0, number_class_6))\n",
    "    percentage_7 = float(get_match_percentage(number_class_0, number_class_7))\n",
    "    percentage_8 = float(get_match_percentage(number_class_0, number_class_8))\n",
    "    percentage_9 = float(get_match_percentage(number_class_0, number_class_9))\n",
    "    percentage_10 = float(get_match_percentage(number_class_0, number_class_10))\n",
    "    percentage_11 = float(get_match_percentage(number_class_0, number_class_11))\n",
    "    percentage_12 = float(get_match_percentage(number_class_0, number_class_12))\n",
    "    \n",
    "    if number_class_0==0:\n",
    "        category='None'\n",
    "    else:\n",
    "        percentages = [percentage_1, percentage_2, percentage_3, percentage_4, percentage_5,\n",
    "               percentage_6, percentage_7, percentage_8, percentage_9, percentage_10,\n",
    "               percentage_11, percentage_12]\n",
    "        max_percentage = max(percentages)\n",
    "        category = cartegories[percentages.index(max_percentage)]\n",
    "    return category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2740d6c2-7e51-44b6-944d-0b86f746147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Página de Inicio\n",
    "home_page_keywords = [\"banca\", \"home\", \"promociones\", \"productos\", \"financieros\", \"bancarios\", \"servicios\", \"noticias\", \"banco\", \"ahorro\", \"préstamos\"]\n",
    "\n",
    "# Formularios de Inicio de Sesión\n",
    "login_keywords = [\"sesión\", \"login\", \"usuario\", \"contraseña\", \"password\", \"acceso\", \"credenciales\", \"identificador\", \"entrar\"]\n",
    "\n",
    "# Formularios de cierre de Sesión\n",
    "logout_keywords = [\"sesión\", \"logout\", \"cierre\", \"salir\"]\n",
    "\n",
    "# Sección de Registro\n",
    "register_keywords = [\"registro\", \"crear\", \"personal\", \"datos\", \"abrir\"]\n",
    "\n",
    "# Información de Productos y Servicios\n",
    "services_keywords = [\"ahorro\", \"cuenta\", \"corrientes\", \"préstamos\", \"tarjetas\", \"casa\", \"nómina\", \"fondos\", \"bolsa\", \"mercados\", \"sostenibilidad\", \"depósitos\",\n",
    "                     \"crédito\", \"inversiones\", \"hipotecas\", \"donaciones\", \"jubilación\", \"inversión\", \"renting\", \"seguros\", \"salud\", \"vida\", \"hogar\", \"coche\", \"moto\"]\n",
    "\n",
    "# Calculadoras Financieras\n",
    "computer_keywords = [\"calculadora\", \"tasa\", \"interés\", \"pagos\", \"finanzas\"]\n",
    "\n",
    "# Zona de Usuarios Registrados\n",
    "profile_keywords = [\"acceso\", \"historial\", \"estado\", \"perfil\", \"configuración\"]\n",
    "\n",
    "# Formularios en Línea\n",
    "form_keywords = [\"transferencia\", \"error\", \"bad\", \"failed\", \"fondo\", \"pago\", \"préstamos\", \"form\", \"formulario\", \"actualización\",\n",
    "'input', 'rellenar', 'enviar', 'send']\n",
    "\n",
    "# Banners y Anuncios\n",
    "banners_keywords = [\"ofertas\", \"promociones\", \"anuncios\", \"destacados\", \"noticias\", \"alertas\"]\n",
    "\n",
    "# Información de Contacto\n",
    "contact_keywords = [\"teléfono\", \"correo\", \"electrónico\", \"atención\", \"sucursales\", \"comunicación\", \"contacto\",]\n",
    "\n",
    "# Seguridad\n",
    "security_keywords = [\"seguridad\", \"privacidad\", \"protección\"]\n",
    "\n",
    "\n",
    "# Enlaces a Redes Sociales\n",
    "social_networks_keywords = [\"facebook\", \"twitter\", \"linkedIn\", \"redes\", \"sociales\", \"youtube\", \"instagram\"]\n",
    "\n",
    "# Broker\n",
    "broker_app_keywords = [\"bróker\", \"broker\"]\n",
    "\n",
    "# Móviles\n",
    "mobile_app_keywords = [\"móvil\", \"android\", \"mobile\", \"imagin\", \"apple\", \"app\"]\n",
    "\n",
    "keywords= home_page_keywords + logout_keywords + login_keywords + register_keywords + services_keywords + computer_keywords + profile_keywords + form_keywords +\\\n",
    "         banners_keywords + contact_keywords  + security_keywords + social_networks_keywords + mobile_app_keywords \n",
    "cartegories = [\"Inicio\", \"Login\", \"Logout\", \"Registro\", \"Servicios\", \"Calculadora\", \"Perfil\", \"Formulario\", \"Banner\", \"Contacto\", \"Seguridad\", \"Redes sociales\", \"Móvil\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6922c5ec-3030-4973-bfc8-da7a5cfdaa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Página de Inicio\n",
    "home_page_keywords = [\"banca\", \"home\", \"promociones\", \"productos\", \"financieros\", \"bancarios\", \"servicios\", \"noticias\", \"banco\", \"ahorro\", \"préstamos\"]\n",
    "\n",
    "# Formularios de Inicio de Sesión\n",
    "login_keywords = [\"sesión\", \"login\", \"usuario\", \"contraseña\", \"password\", \"acceso\", \"credenciales\", \"identificador\", \"entrar\"]\n",
    "\n",
    "# Formularios de cierre de Sesión\n",
    "logout_keywords = [\"sesión\", \"logout\", \"cierre\", \"salir\"]\n",
    "\n",
    "# Sección de Registro\n",
    "register_keywords = [\"registro\", \"crear\", \"personal\", \"datos\", \"abrir\"]\n",
    "\n",
    "# Información de Productos y Servicios\n",
    "services_keywords = [\"ahorro\", \"cuenta\", \"corrientes\", \"préstamos\", \"tarjetas\", \"casa\", \"nómina\", \"fondos\", \"bolsa\", \"mercados\", \"sostenibilidad\", \"depósitos\",\n",
    "                     \"crédito\", \"inversiones\", \"hipotecas\", \"donaciones\", \"jubilación\", \"inversión\", \"renting\", \"seguros\", \"salud\", \"vida\", \"hogar\", \"coche\", \"moto\"]\n",
    "\n",
    "\n",
    "all_keywords_processor=KeywordProcessor()\n",
    "for word in keywords:\n",
    "    all_keywords_processor.add_keyword(word)\n",
    "    \n",
    "home_page_keywords_processor = KeywordProcessor()\n",
    "for word in home_page_keywords:\n",
    "    home_page_keywords_processor.add_keyword(word)\n",
    "    \n",
    "login_keywords_processor =KeywordProcessor()\n",
    "for word in login_keywords:\n",
    "    login_keywords_processor.add_keyword(word)\n",
    "\n",
    "logout_keywords_processor =KeywordProcessor()\n",
    "for word in logout_keywords:\n",
    "    logout_keywords_processor.add_keyword(word)\n",
    "    \n",
    "register_keywords_processor=KeywordProcessor()\n",
    "for word in register_keywords:\n",
    "    register_keywords_processor.add_keyword(word)\n",
    "\n",
    "services_keywords_processor=KeywordProcessor()\n",
    "for word in services_keywords:\n",
    "    services_keywords_processor.add_keyword(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4e5e1d-87fd-4c14-b4be-568847e9cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords_processor=KeywordProcessor()\n",
    "for word in keywords:\n",
    "    all_keywords_processor.add_keyword(word)\n",
    "    \n",
    "home_page_keywords_processor = KeywordProcessor()\n",
    "for word in home_page_keywords:\n",
    "    home_page_keywords_processor.add_keyword(word)\n",
    "    \n",
    "login_keywords_processor =KeywordProcessor()\n",
    "for word in login_keywords:\n",
    "    login_keywords_processor.add_keyword(word)\n",
    "\n",
    "logout_keywords_processor =KeywordProcessor()\n",
    "for word in logout_keywords:\n",
    "    logout_keywords_processor.add_keyword(word)\n",
    "    \n",
    "register_keywords_processor=KeywordProcessor()\n",
    "for word in register_keywords:\n",
    "    register_keywords_processor.add_keyword(word)\n",
    "\n",
    "services_keywords_processor=KeywordProcessor()\n",
    "for word in services_keywords:\n",
    "    services_keywords_processor.add_keyword(word)\n",
    "\n",
    "computer_keyword_processor = KeywordProcessor()\n",
    "for word in computer_keywords:\n",
    "    computer_keyword_processor.add_keyword(word)\n",
    "    \n",
    "profile_keyword_processor = KeywordProcessor()\n",
    "for word in profile_keywords:\n",
    "    profile_keyword_processor.add_keyword(word)\n",
    "    \n",
    "form_keyword_processor = KeywordProcessor()\n",
    "for word in form_keywords:\n",
    "    form_keyword_processor.add_keyword(word)\n",
    "    \n",
    "banners_keyword_processor = KeywordProcessor()\n",
    "for word in banners_keywords:\n",
    "    banners_keyword_processor.add_keyword(word)\n",
    "    \n",
    "contact_keyword_processor = KeywordProcessor()\n",
    "for word in contact_keywords:\n",
    "    contact_keyword_processor.add_keyword(word)\n",
    "    \n",
    "security_keyword_processor = KeywordProcessor()\n",
    "for word in security_keywords:\n",
    "    security_keyword_processor.add_keyword(word)\n",
    "    \n",
    "social_networks_keyword_processor = KeywordProcessor()\n",
    "for word in social_networks_keywords:\n",
    "    social_networks_keyword_processor.add_keyword(word)\n",
    "    \n",
    "mobile_app_keyword_processor = KeywordProcessor()\n",
    "for word in mobile_app_keywords:\n",
    "    mobile_app_keyword_processor.add_keyword(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a10ed9f-ffe9-4b1c-920d-5027dcc351fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xxx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxxx\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xxx' is not defined"
     ]
    }
   ],
   "source": [
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dfc8c4-3746-424c-8e4b-9e870e8ec23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_classficaion_list = []\n",
    "processed_url = {}\n",
    "# Recorrer los test\n",
    "for hit in unique_idTest_list[\"aggregations\"][\"unique_idTest\"]['buckets']:\n",
    "    idTest = hit['key']\n",
    "    # Recorrer los test\n",
    "    data_current_df = get_raw_asm_test_data(es, index_name, idTest, datetime_ini_str, datetime_fi_str)\n",
    "    \n",
    "    for index, row in data_current_df.iterrows():\n",
    "        # Filtrar las filas que tengan valor para el campo urlFailed\n",
    "        if row['urlFailed'] != \"\":\n",
    "            main_url = row['urlFailed']\n",
    "            base_url =  main_url.split('?')[0]\n",
    "            # Si todavía no se ha procesado la url, se procesa\n",
    "            if not base_url in processed_url.keys():\n",
    "                t0 = time.time()\n",
    "                try:\n",
    "                    # Recogemos el html, gurdando el tiempo de respuesta para detenr el programa temporalmente en función del mismo\n",
    "                    # De esta forma se evita saturar la web consultada\n",
    "                    url_response = requests.get(main_url)\n",
    "                    url_response = requests.get(main_url, verify=False, cookies=url_response.cookies)\n",
    "                    response_delay = time.time() - t0\n",
    "                except:\n",
    "                    print(\"error\")\n",
    "                    break\n",
    "                # Obtener el contenido\n",
    "                soup = BeautifulSoup(url_response.content)\n",
    "                main_div = soup.find_all(class_=\"main\")\n",
    "                if(len(main_div) > 0):\n",
    "                    main_div = main_div[0]\n",
    "                else:\n",
    "                    main_div = soup\n",
    "                texts = main_div.findAll(text=True)\n",
    "                text_from_html = ' '.join(texts)\n",
    "                # Clasificar según las categorías\n",
    "                category = get_text_category(text_from_html)\n",
    "                url_classficaion_dict = {\n",
    "                    \"tokenized_source\": text_from_html[0:800],\n",
    "                    \"category\": category,\n",
    "                    \"url\": main_url\n",
    "                }\n",
    "                processed_url[base_url] = category\n",
    "                url_classficaion_list.append(url_classficaion_dict)\n",
    "    \n",
    "                time.sleep(1 * response_delay)\n",
    "\n",
    "\n",
    "\n",
    "print(\"------END-------\")\n",
    "today = str(datetime.today().strftime('%Y-%m-%d'))\n",
    "url_classficaion_df = pd.DataFrame(url_classficaion_list)\n",
    "url_classficaion_df.to_csv(f\"url_classficaion_data_{today}_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b1200-769d-44bb-b614-41d46af209f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = [\n",
    "    \"https://www.caixabank.es/particular/prestamos-personales.html\",\n",
    "    \"https://www.caixabank.es/particular/hipotecas.html\",\n",
    "    \"https://www.caixabank.es/particular/tarjetas-bancarias.html\",\n",
    "    \"https://www.caixabank.es/particular/ahorro.html\",\n",
    "    \"https://www.caixabank.es/particular/prestamos-personales.html\",\n",
    "    \"https://www.caixabank.es/particular/hipotecas.html\",\n",
    "    \"https://www.caixabank.es/particular/tarjetas-bancarias.html\",\n",
    "    \"https://www.caixabank.es/particular/ahorro.html\",\n",
    "    \"https://www.caixabank.es/particular/cuentas-bancarias.html\",\n",
    "    \"https://www.caixabank.es/particular/planes-de-pensiones.html\",\n",
    "    \"https://www.caixabank.es/particular/fondos-de-inversion.html\",\n",
    "    \"https://www.caixabank.es/particular/bolsa-mercados.html\",\n",
    "    \"https://www.caixabank.es/particular/sostenibilidad.html\",\n",
    "    \"https://www.caixabank.es/particular/seguros.html\",\n",
    "    \"https://www.caixabank.es/particular/seguros/seguros-hogar.html\",\n",
    "    \"https://www.caixabank.es/particular/seguros/seguros-vida.html\",\n",
    "    \"https://www.caixabank.es/particular/seguros/seguros-coche-moto.html\",\n",
    "    \"https://www.caixabank.es/particular/seguros/seguros-salud-adeslas.html\",\n",
    "    \"https://www.caixabank.es/particular/seguros/seguros-ahorro.html\",\n",
    "    \"https://www.caixabank.es/particular/movilidad.html\"\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe336b75-62d8-4977-87eb-9528f23820df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "#main_url= \"https://www.rtve.es/play/\"\n",
    "main_url = \"https://www.generali.es/landings/seguros-generali.html?client=false&section=section-no-client&&utm_source=google&utm_medium=cpc&utm_campaign=ES_ES_AO_ADW_._MARCA-PURA_._._MARCA-PURA_._._SEARCH_MARCA-PURA_MARCA-PURA&utm_term=generali%20seguros&gad_source=1&gclid=CjwKCAiA1MCrBhAoEiwAC2d64e6IkP7L40Jj4Nm1KPe8XS3yTBsi3hon3IRwWDv2nOJa_oKJ-LZ9JxoCLxUQAvD_BwE&gclsrc=aw.ds\"\n",
    "#main_url = \"https://catsalut.gencat.cat/ca/inici/\"\n",
    "\n",
    "url_response = requests.get(main_url)\n",
    "url_response = requests.get(main_url, verify=False, cookies=url_response.cookies)\n",
    "\n",
    "soup = BeautifulSoup(url_response.content)\n",
    "\n",
    "# Encuentra todos los elementos 'a' (enlaces) con el atributo 'href'\n",
    "all_links = soup.find_all('a', href=True)\n",
    "\n",
    "# Extrae las URLs de los enlaces\n",
    "urls = [link['href'] for link in all_links]\n",
    "#urls = ['https://www.caixabank.es' + url if url.startswith('/') else url for url in urls]\n",
    "\n",
    "print(len(urls))\n",
    "urls = [url for url in urls if 'https' in url]\n",
    "print(len(urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5db4a305-a3ef-4e71-8963-8aed28d52554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://secure2.rtve.es/usuarios/contenidos/mi-tv/',\n",
       " 'https://secure2.rtve.es/usuarios/contenidos/mi-radio/',\n",
       " 'https://secure2.rtve.es/usuarios/cuenta/mis-datos/',\n",
       " 'https://secure2.rtve.es/usuarios/cuenta/mis-perfiles/',\n",
       " 'https://secure2.rtve.es/usuarios/cuenta/mi-suscripcion/',\n",
       " 'https://secure2.rtve.es/usuarios/acceso/login-tv/',\n",
       " 'https://www.rtve.es/play/videos/duos-increibles/',\n",
       " 'https://www.rtve.es/play/videos/duos-increibles/programa-10-final/7024226/',\n",
       " 'https://secure2.rtve.es/usuarios/acceso/login/']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa32760-be4e-4ee6-beaa-86f4d16411ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_spanish = set(stopwords.words('spanish'))\n",
    "\n",
    "\n",
    "def clean_html_data(text):\n",
    "    # Eliminar saltos de línea\n",
    "    text = text.replace('\\r\\n', ' ')\n",
    "    text = text.replace('\\n\\n', ' ') \n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ') \n",
    "\n",
    "    # Eliminar espacios innecesarios\n",
    "    text = re.sub('\\s\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar caracteres\n",
    "    #text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'[^\\w\\sáéíóúÁÉÍÓÚüÜñÑ]', '', text)\n",
    "\n",
    "    # ELiminar HTML PUBLIC W3CDTD HTML 40 TransitionalEN\n",
    "    text = text.replace('HTML PUBLIC W3CDTD HTML 40 TransitionalEN', '')\n",
    "    # Pasar a minusculas\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove storpwords \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords_spanish]\n",
    "    \n",
    "    # Join the filtered words to form the processed text\n",
    "    processed_text = ' '.join(filtered_words)\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e4a6192-b4cb-488d-90b7-a392a40c3b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    html tv html cabecera nueva rtve lista inicio ...\n",
      "2    html radio html cabecera nueva rtve lista inic...\n",
      "3    html edición cuenta html cabecera nueva rtve c...\n",
      "4                                            not found\n",
      "5    html suscripción rtveplay html cabecera nueva ...\n",
      "6    html asocia dispositivo cuenta usuario html ca...\n",
      "7    html if lte ie 9 lista iniciar sesión tv radio...\n",
      "8    html if lte ie 9 lista iniciar sesión tv radio...\n",
      "9    crear cuenta accede contenido rtve continúa vi...\n",
      "Name: tokenized_source, dtype: object\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_source\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Concatenate along rows (axis=0)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([data_1, data_2,data_3, \u001b[43mdata_4\u001b[49m, data_5 ], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[1;32m     13\u001b[0m result_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl_classification_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_4' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_1 = pd.read_csv('url_classficaion_data_2023-12-05_3.csv', encoding='utf-8')\n",
    "data_2 = pd.read_csv('url_classficaion_data_2023-12-05_1.csv', encoding='utf-8')\n",
    "data_3 = pd.read_csv('url_classficaion_data_2023-12-06_1.csv', encoding='utf-8')\n",
    "#data_4 = pd.read_csv('url_classficaion_data_2023-12-06_2.csv', encoding='utf-8')\n",
    "#data_5 = pd.read_csv('url_classficaion_data_2023-12-06_3.csv', encoding='utf-8')\n",
    "print(data_3['tokenized_source'][1:10])\n",
    "\n",
    "# Concatenate along rows (axis=0)\n",
    "result_df = pd.concat([data_1, data_2,data_3, data_4, data_5 ], axis=0, ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "result_df.to_csv('url_classification_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e2f09-189e-4b99-a7ca-9ddc2881aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('url_classification_data_complete.csv', encoding='utf-8')\n",
    "urls = data['url']\n",
    "new_data = []\n",
    "for index, row in data.iterrows():\n",
    "    text = row['tokenized_source']\n",
    "    text_from_html = BeautifulSoup(text, \"lxml\").text\n",
    "    text_from_html = clean_html_data(text_from_html)\n",
    "    dict = {\"tokenized_source\":text_from_html,\n",
    "           \"category\": row['category'], \n",
    "           \"url\": row['url']\n",
    "           }\n",
    "    new_data.append(dict)\n",
    "url_classficaion_df = pd.DataFrame(new_data)\n",
    "url_classficaion_df.to_csv(f\"url_classficaion_data_{today}_3.csv\", index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4862a82-4b8f-4441-b3e1-40d9990cab6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KO\n",
      "KO\n",
      "------END-------\n"
     ]
    }
   ],
   "source": [
    "url_classficaion_list = []\n",
    "processed_url = {}\n",
    "\n",
    "for main_url in urls:\n",
    "    base_url =  main_url.split('?')[0]\n",
    "    if not base_url in processed_url.keys():\n",
    "        #print(\"New url\")\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            url_response = requests.get(main_url)\n",
    "            url_response = requests.get(main_url, verify=False, cookies=url_response.cookies)\n",
    "            response_delay = time.time() - t0\n",
    "        except:\n",
    "            print(\"error\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(url_response.content)\n",
    "        \n",
    "        main_div = soup.find_all(class_=\"main\")\n",
    "        if(len(main_div) > 0):\n",
    "            main_div = main_div[0]\n",
    "        else:\n",
    "            main_div = soup\n",
    "        #texts = main_div.get_text(separator='')\n",
    "        texts = main_div.findAll(text=True)\n",
    "        text_from_html = ' '.join(texts)\n",
    "        text_from_html = BeautifulSoup(text_from_html, \"lxml\").text\n",
    "        text_from_html = clean_html_data(text_from_html)\n",
    "        if(len(text_from_html) > 20000):\n",
    "            print(\"KO\")\n",
    "            text_from_html = text_from_html[0:20000]\n",
    "        category = get_text_category(text_from_html)\n",
    "        url_classficaion_dict = {\n",
    "            \"tokenized_source\": text_from_html,\n",
    "            \"category\": category,\n",
    "            \"url\": main_url\n",
    "        }\n",
    "        processed_url[base_url] = category\n",
    "        url_classficaion_list.append(url_classficaion_dict)\n",
    "\n",
    "        #print(f\"sleep {response_delay}\")\n",
    "        time.sleep(1 * response_delay)\n",
    "\n",
    "\n",
    "\n",
    "print(\"------END-------\")\n",
    "today = str(datetime.today().strftime('%Y-%m-%d'))\n",
    "url_classficaion_df = pd.DataFrame(url_classficaion_list)\n",
    "url_classficaion_df.to_csv(f\"url_classficaion_data_{today}_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1df87-5cb3-4084-aa9d-fa7a8026c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_idTest_list[\"aggregations\"][\"unique_idTest\"]['buckets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea15e2-fc27-41c7-8aa9-e0f14e5a3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9877f3-1248-4ca3-af78-30c1aa496efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_classficaion_df = pd.DataFrame(url_classficaion_list)\n",
    "url_classficaion_df.to_csv(\"url_classficaion_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb9c23-2dae-4c5d-aa40-710f86c5a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = url_classficaion_df[pd.notnull(url_classficaion_df['tokenized_source'])]\n",
    "data_df = data_df[data_df.category!='None']\n",
    "len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064b534-998d-4b1b-a485-722681d50454",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for index,row in data_df.iterrows():\n",
    "    train_data.append({\"class\":row[\"category\"], \"sentence\":row[\"tokenized_source\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1476cde5-d867-4e38-aa94-f14f1f04a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our training data\n",
    "for pattern in train_data:\n",
    "    # tokenize each word in the sentence\n",
    "    w = nltk.word_tokenize(pattern['sentence'])\n",
    "    # add to our words list\n",
    "    words.extend(w)\n",
    "    # add to documents in our corpus\n",
    "    documents.append((w, pattern['class']))\n",
    "    # add to our classes list\n",
    "    if pattern['class'] not in classes:\n",
    "        classes.append(pattern['class'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = list(set(words))\n",
    "\n",
    "# remove duplicates\n",
    "classes = list(set(classes))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa35e2-d54a-4cd2-a311-c1bdd1336d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a variable\n",
    "main_url = \"https://www4.caixabank.es/apl/donativos/solicitarCargo_es.html?stateName=showImporteYOpcionFiscal&JSESSIONID=kc1mke96lm6V5XKhujYMguO&tkn_ctx=Npx11Be6RG9svIKgT6GzpTW0Tbt93teBeeXBT3mAeTSSyXu5SH\"\n",
    "url_response = requests.get(main_url)\n",
    "url_response = requests.get(main_url, verify=False, cookies=url_response.cookies)\n",
    "#create a beautifulsoup object\n",
    "soup = BeautifulSoup(url_response.content)\n",
    "#print soup\n",
    "#print(mark_soup.prettify())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be1ca0-5cd4-47e2-bd91-3e15a525fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_div = soup_html.find_all(class_=\"main\")[0]\n",
    "texts = main_div.findAll(text=True)                 #find all text\n",
    "text_from_html = ' '.join(texts)\n",
    "catgeory = get_text_category(text_from_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872ab14-67f6-47d7-8bc7-378e6c699275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845be12-f845-4d38-8053-3dbf966cd7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a46f59-ed0b-4e8c-845e-e44af08fbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_class:\n",
    "    \n",
    "        \n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6ebef-82e7-4439-8fee-47162fcee7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_div = soup_html.find_all(class_=\"main\")[0]\n",
    "p_list = []\n",
    "title_list = []\n",
    "href_list = []\n",
    "link_list = []\n",
    "\n",
    "for item in main_div.find_all(['div', 'a']):\n",
    "    #print(\"-----------item\", item)\n",
    "    title_list.append(item.get(\"title\"))\n",
    "    href_list.append(item.select(\"href\"))\n",
    "    text_list.append(item.get_text())\n",
    "    print(item.select(\"p\"))\n",
    "#print(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c511bfc4-353d-4c25-ae51-e62d832600ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_div = soup_html.find_all(class_=\"main\")[0]\n",
    "links_soup = main_div.select(\"a\")\n",
    "title_list = []\n",
    "links_list = []\n",
    "text_list = []\n",
    "for item in links_soup : \n",
    "    print(item)\n",
    "    #title_list.append(item.get(\"title\"))\n",
    "    #links_list.append(item.get(\"href\"))\n",
    "    labels_list.append(item.get_text())\n",
    "#links_list\n",
    "labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a207974-ad82-4dd0-88ed-371ab040a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "links_list = []\n",
    "\n",
    "for eachitem in soup_html.find_all(class_=\"main\") :    \n",
    "    title_list.append(eachitem.get(\"title\"))\n",
    "    links_list.append(eachitem.get(\"href\"))\n",
    "title_list\n",
    "links_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41defb4-bc3f-4917-b326-6975d4eed5e6",
   "metadata": {},
   "source": [
    "# Procesamiento de HARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ce00dc-073b-4fa7-a5f4-19403b8cc148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traducir_codigo_estado(codigo_estado):\n",
    "    try:\n",
    "        # Obtener el mensaje asociado al código de estado\n",
    "        mensaje = HTTPStatus(codigo_estado).phrase\n",
    "        return f'{codigo_estado} {mensaje}'\n",
    "    except ValueError:\n",
    "        # En caso de que el código de estado no sea válido\n",
    "        return f'Código de estado no válido: {codigo_estado}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11adb22-9c7c-448c-84b5-864c4af8b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = '/data/asm-code/asm_kafka_collector/hars'\n",
    "\n",
    "# Especificar el patrón de búsqueda (puedes usar '*' para todos los archivos)\n",
    "patron = '*.har'  # Por ejemplo, busca todos los archivos con extensión .txt\n",
    "\n",
    "# Construir la ruta completa del patrón\n",
    "patron_completo = os.path.join(directorio, patron)\n",
    "\n",
    "# Obtener la lista de archivos que coinciden con el patrón\n",
    "archivos = glob.glob(patron_completo)\n",
    "print(\"patron_completo\", patron_completo)\n",
    "har_list = []\n",
    "# Iterar sobre la lista de archivos\n",
    "for archivo in archivos:\n",
    "    #print(\"archivo\", archivo)\n",
    "    with open(archivo, 'r') as f:\n",
    "        har_parser = HarParser(json.loads(f.read())['HARFile'])\n",
    "    data = har_parser.har_data\n",
    "    print(archivo)\n",
    "    entry_data = har_parser.har_data[\"entries\"][0]\n",
    "    #print(entry_data.keys())\n",
    "    #print(entry_data)\n",
    "    #print(entry_data[\"response\"][\"status\"], traducir_codigo_estado(entry_data[\"response\"][\"status\"]))\n",
    "    har_data_dict= {\n",
    "        \"server_response_code\": entry_data[\"response\"][\"status\"],\n",
    "        \"server_response_message\": traducir_codigo_estado(entry_data[\"response\"][\"status\"]),\n",
    "        \"status_text\": entry_data[\"response\"][\"statusText\"],\n",
    "        \"url\": entry_data[\"request\"][\"url\"],\n",
    "        \"date\": entry_data[\"startedDateTime\"].split('.')[0],\n",
    "        \"id\": (archivo.split('/')[-1]).split('.')[0]\n",
    "    }\n",
    "    \n",
    "    #fecha_hora_objeto = datetime.fromisoformat(har_data_dict[\"date\"])\n",
    "    har_list.append(har_data_dict)\n",
    "    #print(har_data_dict)\n",
    "    \n",
    "    \n",
    "har_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdfd5b7-327b-46b2-9317-9ddd5e4d2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('sample.har', 'r') as f:\n",
    "    har_parser = HarParser(json.loads(f.read()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
